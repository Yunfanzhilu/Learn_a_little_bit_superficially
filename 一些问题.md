>我们都是站在巨人的肩膀上~~~~
>
>唯有热爱可抵岁月漫长~~~

##  1.PCA 数据降维和线性回归有什么区别？通俗举例说明

>```python
>PCA（主成分分析）数据降维
>(1)目的：
>PCA 主要目的是在尽量保留数据原有信息的前提下，降低数据的维度。
>(2)原理通俗解释：
>想象你有一群学生的各科成绩数据，比如语文、数学、英语、物理、化学等多门课程的成绩。这些数据是高维的（有很多个学科维度）。PCA 就像是找到这些成绩数据背后的一些 “隐藏模式” 或者 “综合特征”。例如，可能存在一种 “综合学习能力” 的模式，有些学生在所有科目上都表现较好或者较差，这是一种综合的特征；还有一种可能是 “偏文科能力” 和 “偏理科能力” 这样的特征。PCA 就是把原来很多学科的成绩数据，转化成几个主要的综合特征来表示，从而降低了数据的维度。
>(3)举例：
>假设我们有一个数据集是关于 100 个学生的 5 门课程（语文、数学、英语、物理、化学）的成绩。每个学生就是一个数据点，每门课程就是一个维度，所以原始数据是 100 个数据点在 5 维空间中。通过 PCA 分析后，可能发现可以用 2 个主要的 “综合特征”（比如综合学习能力和文理科偏向）来很好地表示这些学生的成绩情况。这样就把 5 维的数据降到了 2 维，方便后续的分析、可视化等操作。
>
>线性回归
>(1)目的:
>线性回归是要找到一个自变量（一个或多个）与因变量之间的线性关系，用于预测或者解释因变量的变化。
>(2)原理通俗解释
>还是以学生成绩为例，假设我们想知道学生的数学成绩（因变量）和他们每天花在数学学习上的时间（自变量）之间的关系。线性回归就是要找到一条直线（在单自变量情况下）或者一个超平面（在多自变量情况下），使得根据学生花在数学学习上的时间能够尽可能准确地预测他们的数学成绩。
>(3)举例
>比如我们收集了 50 个学生的数据，记录了他们每天花在数学学习上的时间（从 1 小时到 5 小时不等）以及他们对应的数学考试成绩。线性回归就是要找到一个类似 “数学成绩 = a学习时间 + b”（这里 a 和 b 是要确定的系数）这样的关系。如果发现 a = 10（表示每多学习 1 小时，成绩提高 10 分），b = 50（表示即使不学习也有 50 分的基础分），那么就可以根据这个关系预测其他学生如果每天花 3 小时学习数学，他们的数学成绩大概是 103+50 = 80 分。
>
>两者区别总结
>PCA 关注的是数据内部的结构关系，是一种无监督的方法，目的是简化数据结构、降低维度，不需要有因变量和自变量的区分。而线性回归是有监督的学习方法，重点在于找到自变量和因变量之间的线性关系，用于预测或者解释因变量的值。
>```
>
>

## 2.if __name__ == 'main' 请问为什么经常可以看调用的时候看到这段话？

>```python
>一个python文件通常有两种使用方法，第一是作为脚本直接执行，
># 第二是 import 到其他的 python 脚本中被调用（模块重用）执行。
># 因此 if __name__ == 'main': 的作用就是控制这两种情况执行代码的过程，
># 在 if __name__ == 'main': 下的代码只有在第一种情况下（即文件作为脚本直接执行）才会被执行，
># 而 import 到其他脚本中是不会被执行的。
>```

## 3.AI的分类

AI->机器学习->表示学习(特征提取)->深度学习

![img](/Users/hcb/Desktop/PYTHON/Learn_a_little_bit_superficially/PyTorch专项学习/AI分类.jpeg)

## 4.python面向对象继承问题
>```python
>在Python中，当你定义一个类并继承自另一个类时，你不需要显式地调用父类的构造函数来初始化父类。
>继承会自动完成这一过程。然而，在PyTorch中，由于nn.Module是一个特殊的类， 
>它包含了一些重要的初始化代码，比如参数注册等，所以你需要显式地调用super().__init__()来确保这些初始化过程被执行。

Python 类继承中的构造函数调用基础
默认的继承机制：
在 Python 中，当定义一个子类继承自另一个父类时，子类确实会自动调用父类的构造函数（在某些情况下）。例如，下面是一个简单的 Python 类继承示例：

```python
class Animal:
    def __init__(self, name):
        self.name = name

    def speak(self):
        print(f"{self.name} makes a sound")


class Dog(Animal):
    def bark(self):
        print(f"{self.name} barks")
my_dog = Dog("Buddy")
my_dog.bark()
my_dog.speak()
```

```python
在这个例子中，当创建Dog类的实例my_dog时，虽然在Dog类的定义中没有显式调用Animal类的__init__构造函数，但 Python 会自动沿着继承链去调用父类的__init__，使得Dog实例能够正确地初始化name这个属性，进而可以调用Animal类中定义的speak方法。这是 Python 提供的一种便捷机制，它遵循一定的方法解析顺序（MRO，Method Resolution Order）来查找和调用合适的构造函数及其他方法。
存在的限制和特殊情况：
然而，如果父类的构造函数需要特定的参数传递，而子类没有正确处理这些参数的传递，就可能出现问题。例如：

class Animal:
    def __init__(self, name, age):
        self.name = name
        self.age = age

    def speak(self):
        print(f"{self.name} of age {self.age} makes a sound")


class Dog(Animal):
    def bark(self):
        print(f"{self.name} barks")
my_dog = Dog("Buddy") 
```

***这里会报错，因为没有传递age参数给父类的构造函数***
在上述代码中，Animal类的构造函数需要两个参数name和age，但在Dog类实例化时，没有正确传递age参数，就会导致报错。所以即使有默认的自动调用机制，在实际使用中，为了更明确地控制参数传递等初始化行为，有时候也需要在子类中显式地调用父类的构造函数，像这样修改Dog类：

```python
class Dog(Animal):
    def __init__(self, name, age):
        super().__init__(name, age)

    def bark(self):
        print(f"{self.name} barks")

my_dog = Dog("Buddy", 3)
my_dog.bark()
my_dog.speak()
```

这里通过super().__init__(name, age)显式调用了父类Animal的构造函数，保证了正确的初始化。

PyTorch 中nn.Module类继承时显式调用构造函数的原因
nn.Module类的重要性及作用：
在 PyTorch 中，nn.Module是构建神经网络模型的基类，几乎所有自定义的神经网络模块都会继承自它。它承担了诸多重要功能，比如管理模型的参数（包括参数的注册、存储等），使得这些参数能够被正确地识别和处理，例如在进行模型训练时，优化器可以方便地找到所有需要更新的可学习参数；同时它也处理了模块的状态管理、前向传播的逻辑组织等关键方面。
参数注册机制依赖于构造函数初始化：
nn.Module类内部在其构造函数__init__中包含了对参数进行注册等重要的初始化操作代码。例如，当我们在自定义神经网络层（继承自nn.Module）中定义了一些可学习的参数（像nn.Linear层中的权重和偏置等），这些参数需要通过nn.Module的相关机制进行注册，才能被 PyTorch 的框架正确识别并参与后续的计算和更新过程。以下是一个简单的自定义线性层的例子：

```python
import torch
import torch.nn as nn

class MyLinear(nn.Module):
    def __init__(self, in_features, out_features):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(out_features, in_features))
        self.bias = nn.Parameter(torch.randn(out_features))

    def forward(self, x):
        return torch.matmul(x, self.weight.t()) + self.bias

my_linear = MyLinear(10, 5)
```

在这个自定义的MyLinear类中，继承自nn.Module，如果没有显式地调用super().__init__()，那么self.weight和self.bias这些参数就不会被正确地注册到 PyTorch 的参数管理系统中，后续在使用这个自定义层构建模型并进行训练时（例如将其放入nn.Sequential容器中，再配合优化器进行训练等操作），就会出现问题，比如优化器无法找到这些参数来进行更新，导致模型无法正常训练。
确保继承链上的完整初始化：
除了参数注册外，nn.Module类的构造函数可能还会执行其他对整个模块状态和功能有重要影响的初始化任务。通过显式调用super().__init__()，可以保证沿着继承链，不仅是当前自定义类中定义的初始化操作得以执行，而且父类（nn.Module以及可能它之上更顶层的父类，如果存在多层继承的话）中的初始化逻辑都能依次正确执行，从而保证整个继承体系下模块的完整性和正确性，使其能在 PyTorch 的生态环境（如模型训练、评估、部署等各个环节）中正常工作。
总之，在 Python 一般的类继承中虽有自动调用父类构造函数的机制，但在 PyTorch 里针对nn.Module这个特殊且关键的类进行继承时，为了确保像参数注册等重要初始化过程顺利完成，必须显式调用super().__init__()，以符合 PyTorch 框架对模型构建和使用的要求。